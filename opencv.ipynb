{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Video Processing with Computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/image.jpeg')\n",
    "cv2.imshow('Original Imagee', image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/image.jpg')\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray Image', gray_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/image.jpeg')\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred_img = cv2.GaussianBlur(gray_image, (5, 5), 100)\n",
    "cv2.imshow('Blurred Image', blurred_img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/image.jpg')\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred_img = cv2.GaussianBlur(gray_image, (5, 5), 100)\n",
    "edges = cv2.Canny(blurred_img, 50, 150)\n",
    "cv2.imshow('Edge Detection', edges)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'TrackerKCF_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m num_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Adjust this value based on your specific use case\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create KCF trackers\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m trackers \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrackerKCF_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_objects\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Read the first frame\u001b[39;00m\n\u001b[0;32m     13\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m num_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Adjust this value based on your specific use case\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create KCF trackers\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m trackers \u001b[38;5;241m=\u001b[39m [\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrackerKCF_create\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_objects)]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Read the first frame\u001b[39;00m\n\u001b[0;32m     13\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerKCF_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('videos/influence.mp4') \n",
    "\n",
    "num_objects = 2 \n",
    "\n",
    "trackers = [cv2.TrackerKCF_create() for _ in range(num_objects)]\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if not ret:\n",
    "    print(\"Error reading the video feed\")\n",
    "    exit()\n",
    "\n",
    "# Select the regions of interest (ROIs) for each object to track\n",
    "# You should set these coordinates based on your specific use case\n",
    "# For example, manually draw bounding boxes around objects or use an object detection model\n",
    "\n",
    "rois = [(x1, y1, w1, h1), (x2, y2, w2, h2)] \n",
    "\n",
    "for i, tracker in enumerate(trackers):\n",
    "    tracker.init(frame, rois[i])\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        ret, bbox = tracker.update(frame)\n",
    "\n",
    "        if ret:\n",
    "            p1 = (int(bbox[0]), int(bbox[1]))\n",
    "            p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "            cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Multi-object Tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processing with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image_path = 'images/image.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "edges = cv2.Canny(blurred_image, 50, 150)\n",
    "\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Edges', edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Image Processing with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image_path = 'images/image.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "edges = cv2.Canny(blurred_image, 50, 150)\n",
    "\n",
    "contours, _ = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Edges', edges)\n",
    "cv2.imshow('Contours', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tracker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Update the tracker with the new frame\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m ret, bbox \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241m.\u001b[39mupdate(frame)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m# Draw a rectangle around the tracked object\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     p1 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m(bbox[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(bbox[\u001b[38;5;241m1\u001b[39m]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tracker' is not defined"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    ret, bbox = tracker.update(frame)\n",
    "\n",
    "    if ret:\n",
    "        p1 = (int(bbox[0]), int(bbox[1]))\n",
    "        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'TrackerKCF_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize the tracker (using KCF tracker in this example)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tracker \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrackerKCF_create\u001b[49m()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Read the first frame from the video stream\u001b[39;00m\n\u001b[0;32m     10\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerKCF_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "tracker = cv2.TrackerKCF_create()\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "bbox = cv2.selectROI(\"Tracking\", frame, fromCenter=False, showCrosshair=True)\n",
    "\n",
    "tracker.init(frame, bbox)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    ret, bbox = tracker.update(frame)\n",
    "\n",
    "    if ret:\n",
    "        p1 = (int(bbox[0]), int(bbox[1]))\n",
    "        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "tracker = cv2.TrackerMIL_create()\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "bbox = cv2.selectROI(\"Tracking\", frame, fromCenter=False, showCrosshair=True)\n",
    "\n",
    "tracker.init(frame, bbox)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    ret, bbox = tracker.update(frame)\n",
    "\n",
    "    if ret:\n",
    "        p1 = (int(bbox[0]), int(bbox[1]))\n",
    "        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "trackers = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if not trackers or cv2.waitKey(1) & 0xFF == ord('a'):\n",
    "        bbox = cv2.selectROI(\"Tracking\", frame, fromCenter=False, showCrosshair=True)\n",
    "\n",
    "        tracker = cv2.TrackerMIL_create()\n",
    "        tracker.init(frame, bbox)\n",
    "\n",
    "        trackers.append(tracker)\n",
    "\n",
    "    for tracker in trackers:\n",
    "        ret, bbox = tracker.update(frame)\n",
    "\n",
    "        if ret:\n",
    "            p1 = (int(bbox[0]), int(bbox[1]))\n",
    "            p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "            cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Processing with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "video_path = 'videos/influence.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Grayscale Frame', gray_frame)\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection with OpenCV and a Pre-trained Model (Using MobileNet SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time Face Detection with OpenCV and Haarcascades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmentation with OpenCV (Using GrabCut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image_path = 'images/image.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "mask = np.zeros(image.shape[:2], np.uint8)\n",
    "\n",
    "rect = (50, 50, image.shape[1] - 50, image.shape[0] - 50)\n",
    "\n",
    "cv2.grabCut(image, mask, rect, None, None, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "\n",
    "result = image * mask2[:, :, np.newaxis]\n",
    "\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Segmented Image', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Subtraction for Moving Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "video_path = 'videos/influence.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Foreground Mask', fgmask)\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical Flow for Motion Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "video_path = 'videos/influence.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "ret, frame1 = cap.read()\n",
    "\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[..., 1] = 255\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    next_frame = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    cv2.imshow('Optical Flow', rgb)\n",
    "\n",
    "    prvs = next_frame\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmentation with a Pre-trained Deep Learning Model (Using DeepLabv3+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Recognition with OpenCV and dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Estimation with OpenCV and PoseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Stitching with OpenCV (Panorama Creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image1 = cv2.imread('images/image.jpg')\n",
    "image2 = cv2.imread('images/image.jpeg')\n",
    "\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "keypoints1, descriptors1 = orb.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "H, _ = cv2.findHomography(points1, points2, cv2.RANSAC, 5.0)\n",
    "\n",
    "result = cv2.warpPerspective(image1, H, (image1.shape[1] + image2.shape[1], image1.shape[0]))\n",
    "\n",
    "result[0:image2.shape[0], 0:image2.shape[1]] = image2\n",
    "\n",
    "cv2.imshow('Image 1', image1)\n",
    "cv2.imshow('Image 2', image2)\n",
    "cv2.imshow('Stitched Panorama', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Tracking with OpenCV (Using Mean Shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Inpainting with OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time Emotion Detection with OpenCV and a Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Recognition with Tesseract OCR and OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Transfer with Neural Style Transfer Algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
