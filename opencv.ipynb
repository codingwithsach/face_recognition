{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Video Processing with Computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processing with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load an image from file\n",
    "image_path = 'images/image.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply a Gaussian blur\n",
    "blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "# Perform edge detection using Canny\n",
    "edges = cv2.Canny(blurred_image, 50, 150)\n",
    "\n",
    "# Display the original and processed images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Edges', edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Image Processing with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load an image from file\n",
    "image_path = 'images/image.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply a Gaussian blur\n",
    "blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "# Perform edge detection using Canny\n",
    "edges = cv2.Canny(blurred_image, 50, 150)\n",
    "\n",
    "# Find contours in the edged image\n",
    "contours, _ = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Draw contours on the original image\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# Display the original, processed, and contoured images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Edges', edges)\n",
    "cv2.imshow('Contours', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Processing with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open a video capture object\n",
    "video_path = 'videos/influence.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Read and process frames from the video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform some processing on each frame (e.g., convert to grayscale)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the original and processed frames\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Grayscale Frame', gray_frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection with OpenCV and a Pre-trained Model (Using MobileNet SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"path/to/MobileNetSSD_deploy.prototxt\" in function 'cv::dnn::ReadProtoFromTextFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\opencv.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Load pre-trained MobileNet SSD model and its configuration\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m net \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mdnn\u001b[39m.\u001b[39;49mreadNetFromCaffe(\u001b[39m'\u001b[39;49m\u001b[39mpath/to/MobileNetSSD_deploy.prototxt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpath/to/MobileNetSSD_deploy.caffemodel\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Open a video capture object\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m video_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvideos/influence.mp4\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1126: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"path/to/MobileNetSSD_deploy.prototxt\" in function 'cv::dnn::ReadProtoFromTextFile'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load pre-trained MobileNet SSD model and its configuration\n",
    "net = cv2.dnn.readNetFromCaffe('path/to/MobileNetSSD_deploy.prototxt', 'path/to/MobileNetSSD_deploy.caffemodel')\n",
    "\n",
    "# Open a video capture object\n",
    "video_path = 'videos/influence.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Define the class labels for MobileNet SSD\n",
    "classes = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\",\n",
    "           \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]\n",
    "\n",
    "# Read and process frames from the video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform object detection using MobileNet SSD\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.007843, (300, 300), 127.5)\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    # Loop over the detections and draw bounding boxes\n",
    "    for i in range(detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        if confidence > 0.2:  # Filter out weak detections\n",
    "            class_id = int(detections[0, 0, i, 1])\n",
    "            label = f\"{classes[class_id]}: {confidence:.2f}%\"\n",
    "\n",
    "            # Draw bounding box and label on the frame\n",
    "            box = detections[0, 0, i, 3:7] * np.array([frame.shape[1], frame.shape[0], frame.shape[1], frame.shape[0]])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "            y = startY - 15 if startY - 15 > 15 else startY + 15\n",
    "            cv2.putText(frame, label, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with object detections\n",
    "    cv2.imshow('Object Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time Face Detection with OpenCV and Haarcascades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained Haarcascades face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Open a video capture object\n",
    "cap = cv2.VideoCapture(0)  # Use the default camera (you can change the parameter to a video file path)\n",
    "\n",
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Read and process frames from the camera\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Perform face detection\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # Draw rectangles around the detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (q255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame with face detections\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmentation with OpenCV (Using GrabCut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load an image from file\n",
    "image_path = 'images/image.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Create a mask (1 for foreground, 2 for probable foreground, 0 for background, 3 for probable background)\n",
    "mask = np.zeros(image.shape[:2], np.uint8)\n",
    "\n",
    "# Define the region of interest (ROI) for segmentation\n",
    "rect = (50, 50, image.shape[1] - 50, image.shape[0] - 50)\n",
    "\n",
    "# Apply GrabCut algorithm\n",
    "cv2.grabCut(image, mask, rect, None, None, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# Modify the mask to create a binary mask (0 and 2 for background, 1 and 3 for foreground)\n",
    "mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "\n",
    "# Multiply the image with the binary mask to get the segmented image\n",
    "result = image * mask2[:, :, np.newaxis]\n",
    "\n",
    "# Display the original and segmented images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Segmented Image', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Subtraction for Moving Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open a video capture object\n",
    "video_path = 'videos/influence.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Create a background subtractor\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Read and process frames from the video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Display the original frame and the foreground mask\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Foreground Mask', fgmask)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical Flow for Motion Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open a video capture object\n",
    "video_path = 'videos/influence.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "\n",
    "# Convert the first frame to grayscale\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[..., 1] = 255\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "\n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the current frame to grayscale\n",
    "    next_frame = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Convert polar coordinates to Cartesian\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Convert HSV to BGR for visualization\n",
    "    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # Display the optical flow\n",
    "    cv2.imshow('Optical Flow', rgb)\n",
    "\n",
    "    # Update the previous frame\n",
    "    prvs = next_frame\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmentation with a Pre-trained Deep Learning Model (Using DeepLabv3+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"path/to/deeplabv3_257_mv_gpu.tflite\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\opencv.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Load a pre-trained DeepLabv3+ model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m net \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mdnn\u001b[39m.\u001b[39;49mreadNetFromTensorflow(\u001b[39m'\u001b[39;49m\u001b[39mpath/to/deeplabv3_257_mv_gpu.tflite\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X25sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Open an image from file\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m image_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mimages/image.jpeg\u001b[39m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"path/to/deeplabv3_257_mv_gpu.tflite\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load a pre-trained DeepLabv3+ model\n",
    "net = cv2.dnn.readNetFromTensorflow('path/to/deeplabv3_257_mv_gpu.tflite')\n",
    "\n",
    "# Open an image from file\n",
    "image_path = 'images/image.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Resize the image to match the input size of the model\n",
    "input_size = (257, 257)\n",
    "input_image = cv2.resize(image, input_size)\n",
    "\n",
    "# Prepare the input image for the model\n",
    "input_blob = cv2.dnn.blobFromImage(input_image, scalefactor=1.0 / 127.5, mean=[127.5, 127.5, 127.5], swapRB=True)\n",
    "\n",
    "# Set the input to the model\n",
    "net.setInput(input_blob)\n",
    "\n",
    "# Run inference\n",
    "output = net.forward()\n",
    "\n",
    "# Get the segmentation mask\n",
    "mask = output[0, 0]\n",
    "\n",
    "# Threshold the mask to create a binary mask\n",
    "_, binary_mask = cv2.threshold(mask, 0.5, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "# Resize the binary mask to the original image size\n",
    "binary_mask = cv2.resize(binary_mask, (image.shape[1], image.shape[0]))\n",
    "\n",
    "# Apply the binary mask to the original image\n",
    "segmented_image = cv2.bitwise_and(image, image, mask=binary_mask)\n",
    "\n",
    "# Display the original image, segmentation mask, and segmented image\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Segmentation Mask', binary_mask)\n",
    "cv2.imshow('Segmented Image', segmented_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Recognition with OpenCV and dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\opencv.ipynb Cell 21\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdlib\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X31sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Load the pre-trained face detector from dlib\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X31sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m detector \u001b[39m=\u001b[39m dlib\u001b[39m.\u001b[39mget_frontal_face_detector()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "\n",
    "# Load the pre-trained face detector from dlib\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Load the pre-trained facial landmarks predictor from dlib\n",
    "predictor = dlib.shape_predictor('path/to/shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Open a video capture object\n",
    "video_path = 'path/to/your/video.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = detector(gray_frame)\n",
    "\n",
    "    for face in faces:\n",
    "        # Get facial landmarks\n",
    "        landmarks = predictor(gray_frame, face)\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (face.left(), face.top()), (face.right(), face.bottom()), (0, 255, 0), 2)\n",
    "\n",
    "        # Draw facial landmarks\n",
    "        for i in range(68):\n",
    "            x, y = landmarks.part(i).x, landmarks.part(i).y\n",
    "            cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)\n",
    "\n",
    "    # Display the frame with face detection and landmarks\n",
    "    cv2.imshow('Facial Recognition', frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Estimation with OpenCV and PoseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"path/to/pose/pose_iter_440000.caffemodel\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\opencv.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     exit()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Load the pre-trained PoseNet model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m net \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mdnn\u001b[39m.\u001b[39;49mreadNetFromTensorflow(\u001b[39m'\u001b[39;49m\u001b[39mpath/to/pose/pose_iter_440000.caffemodel\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mpath/to/pose/pose_deploy_linevec.prototxt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X33sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X33sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     ret, frame \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\caffe\\caffe_io.cpp:1138: error: (-2:Unspecified error) FAILED: fs.is_open(). Can't open \"path/to/pose/pose_iter_440000.caffemodel\" in function 'cv::dnn::ReadProtoFromBinaryFile'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Open a video capture object\n",
    "video_path = 'videos/influence.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Load the pre-trained PoseNet model\n",
    "net = cv2.dnn.readNetFromTensorflow('path/to/pose/pose_iter_440000.caffemodel', 'path/to/pose/pose_deploy_linevec.prototxt')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize the frame to match the input size of the model\n",
    "    input_size = (368, 368)\n",
    "    input_frame = cv2.resize(frame, input_size)\n",
    "\n",
    "    # Prepare the input frame for the model\n",
    "    input_blob = cv2.dnn.blobFromImage(input_frame, 1.0 / 255, input_size, (127.5, 127.5, 127.5), swapRB=True, crop=False)\n",
    "\n",
    "    # Set the input to the model\n",
    "    net.setInput(input_blob)\n",
    "\n",
    "    # Run inference\n",
    "    output = net.forward()\n",
    "\n",
    "    # Draw the keypoints and skeletal structure on the frame\n",
    "    points = output[0, 0, :, :]\n",
    "    for i in range(points.shape[0]):\n",
    "        x = int(points[i, 0] * frame.shape[1])\n",
    "        y = int(points[i, 1] * frame.shape[0])\n",
    "        cv2.circle(frame, (x, y), 5, (0, 255, 255), -1)\n",
    "\n",
    "    # Display the frame with pose estimation\n",
    "    cv2.imshow('Pose Estimation', frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Stitching with OpenCV (Panorama Creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read a series of images for stitching\n",
    "image1 = cv2.imread('images/image.jpg')\n",
    "image2 = cv2.imread('images/image.jpeg')\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect key points and descriptors using ORB\n",
    "orb = cv2.ORB_create()\n",
    "keypoints1, descriptors1 = orb.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "# Use a brute-force matcher to find the best matches\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "# Sort the matches based on their distances\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# Extract the matched key points\n",
    "points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Find the homography matrix\n",
    "H, _ = cv2.findHomography(points1, points2, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Warp the first image to align with the second image\n",
    "result = cv2.warpPerspective(image1, H, (image1.shape[1] + image2.shape[1], image1.shape[0]))\n",
    "\n",
    "# Overlay the second image onto the result\n",
    "result[0:image2.shape[0], 0:image2.shape[1]] = image2\n",
    "\n",
    "# Display the original images and the stitched panorama\n",
    "cv2.imshow('Image 1', image1)\n",
    "cv2.imshow('Image 2', image2)\n",
    "cv2.imshow('Stitched Panorama', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Tracking with OpenCV (Using Mean Shift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not open video.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\opencv.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Define the region of interest (ROI) for tracking\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x, y, w, h \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m, \u001b[39m150\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m100\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m roi \u001b[39m=\u001b[39m frame[y:y \u001b[39m+\u001b[39;49m h, x:x \u001b[39m+\u001b[39;49m w]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X40sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Convert the ROI to the HSV color space\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X40sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m roi_hsv \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(roi, cv2\u001b[39m.\u001b[39mCOLOR_BGR2HSV)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open a video capture object\n",
    "video_path = 'path/to/your/video.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Read the first frame\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Define the region of interest (ROI) for tracking\n",
    "x, y, w, h = 200, 150, 100, 100\n",
    "roi = frame[y:y + h, x:x + w]\n",
    "\n",
    "# Convert the ROI to the HSV color space\n",
    "roi_hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Create a mask and histogram for the ROI\n",
    "roi_hist = cv2.calcHist([roi_hsv], [0], None, [180], [0, 180])\n",
    "roi_hist = cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Set the termination criteria for mean shift\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to the HSV color space\n",
    "    frame_hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Calculate the back projection of the frame using the ROI histogram\n",
    "    dst = cv2.calcBackProject([frame_hsv], [0], roi_hist, [0, 180], 1)\n",
    "\n",
    "    # Apply mean shift to track the object\n",
    "    ret, track_window = cv2.meanShift(dst, (x, y, w, h), term_crit)\n",
    "\n",
    "    # Draw the tracked object on the frame\n",
    "    x, y, w, h = track_window\n",
    "    img2 = cv2.rectangle(frame, (x, y), (x + w, y + h), 255, 2)\n",
    "\n",
    "    # Display the frame with the tracked object\n",
    "    cv2.imshow('Object Tracking', img2)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Inpainting with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read an image with a region to be inpainted\n",
    "image_path = 'path/to/your/image.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "mask = cv2.imread('path/to/your/mask.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Perform inpainting\n",
    "inpainting = cv2.inpaint(image, mask, inpaintRadius=3, flags=cv2.INPAINT_TELEA)\n",
    "\n",
    "# Display the original image, mask, and inpainted result\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Mask', mask)\n",
    "cv2.imshow('Inpainting Result', inpainting)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time Emotion Detection with OpenCV and a Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained emotion detection model\n",
    "model = load_model('path/to/emotion_model.h5')\n",
    "\n",
    "# Open a video capture object\n",
    "cap = cv2.VideoCapture(0)  # Use the default camera\n",
    "\n",
    "# Load the pre-trained Haarcascades face detector\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Define the emotion labels\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to grayscale for face detection\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract the face region\n",
    "        face_roi = gray_frame[y:y + h, x:x + w]\n",
    "        face_roi = cv2.resize(face_roi, (48, 48), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        # Normalize the pixel values\n",
    "        face_roi = face_roi / 255.0\n",
    "\n",
    "        # Reshape the input for the model\n",
    "        face_roi = np.reshape(face_roi, (1, 48, 48, 1))\n",
    "\n",
    "        # Predict emotion\n",
    "        emotion_prediction = model.predict(face_roi)\n",
    "        emotion_label = emotion_labels[np.argmax(emotion_prediction)]\n",
    "\n",
    "        # Draw a rectangle around the face and display the predicted emotion\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, f'Emotion: {emotion_label}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame with face detection and emotion prediction\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Recognition with Tesseract OCR and OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "\n",
    "# Set the path to the Tesseract executable (adjust as needed)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n",
    "\n",
    "# Read an image with text\n",
    "image_path = 'path/to/your/image_with_text.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Use pytesseract to perform OCR\n",
    "text = pytesseract.image_to_string(gray)\n",
    "\n",
    "# Display the original image and recognized text\n",
    "cv2.imshow('Original Image', image)\n",
    "print(f\"Recognized Text:\\n{text}\")\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Transfer with Neural Style Transfer Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.python'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\python\\opencv.ipynb Cell 35\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X46sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X46sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m load_img, img_to_array\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X46sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m \u001b[39mimport\u001b[39;00m vgg19\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/python/opencv.ipynb#X46sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m backend \u001b[39mas\u001b[39;00m K\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m __internal__\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m activations\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m applications\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\__internal__\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m layers\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m__internal__\u001b[39;00m \u001b[39mimport\u001b[39;00m losses\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\__internal__\\backend\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m _initialize_variables \u001b[39mas\u001b[39;00m initialize_variables\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend\u001b[39;00m \u001b[39mimport\u001b[39;00m track_variable\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\__init__.py:21\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m applications\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\applications\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m\"\"\"Keras Applications are premade architectures with pre-trained weights.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconvnext\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvNeXtBase\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconvnext\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvNeXtLarge\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapplications\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconvnext\u001b[39;00m \u001b[39mimport\u001b[39;00m ConvNeXtSmall\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\applications\\convnext.py:26\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m\"\"\"ConvNeXt models for Keras.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[39mReferences:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m  (CVPR 2022)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mv2\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m backend\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msrc\u001b[39;00m \u001b[39mimport\u001b[39;00m initializers\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\__init__.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[0;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m KerasLazyLoader \u001b[39mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.python'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import vgg19\n",
    "from keras import backend as K\n",
    "from keras_contrib.applications import neural_transfer\n",
    "\n",
    "# Set the paths to your content and style images\n",
    "content_image_path = 'images/image.jpg'\n",
    "style_image_path = 'images/image.jpeg'\n",
    "\n",
    "# Load content and style images\n",
    "content_image = load_img(content_image_path, target_size=(400, 400))\n",
    "style_image = load_img(style_image_path, target_size=(400, 400))\n",
    "\n",
    "content_array = img_to_array(content_image)\n",
    "content_array = np.expand_dims(content_array, axis=0)\n",
    "style_array = img_to_array(style_image)\n",
    "style_array = np.expand_dims(style_array, axis=0)\n",
    "\n",
    "# Preprocess input for VGG19 model\n",
    "content_array = vgg19.preprocess_input(content_array)\n",
    "style_array = vgg19.preprocess_input(style_array)\n",
    "\n",
    "# Define content layer and style layers for neural transfer\n",
    "content_layer = 'block5_conv2'\n",
    "style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "# Build the neural transfer model\n",
    "model = neural_transfer.build_model(content_array, style_array, content_layer, style_layers)\n",
    "\n",
    "# Run neural transfer\n",
    "generated_image = neural_transfer.run_transfer(model, content_array, style_array, iterations=100)\n",
    "\n",
    "# Convert the generated image back to a displayable format\n",
    "generated_image = neural_transfer.deprocess_image(generated_image)\n",
    "\n",
    "# Display the content, style, and generated images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(load_img(content_image_path))\n",
    "plt.title('Content Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(load_img(style_image_path))\n",
    "plt.title('Style Image')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(generated_image)\n",
    "plt.title('Generated Image')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
