{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and Video Processing with Computer vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/image.jpeg')\n",
    "cv2.imshow('Original Imagee', image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/image.jpg')\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow('Gray Image', gray_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/image.jpeg')\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred_img = cv2.GaussianBlur(gray_image, (5, 5), 100)\n",
    "cv2.imshow('Blurred Image', blurred_img)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('images/image.jpg')\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred_img = cv2.GaussianBlur(gray_image, (5, 5), 100)\n",
    "edges = cv2.Canny(blurred_img, 50, 150)\n",
    "cv2.imshow('Edge Detection', edges)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'TrackerKCF_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m num_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Adjust this value based on your specific use case\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create KCF trackers\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m trackers \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrackerKCF_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_objects\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Read the first frame\u001b[39;00m\n\u001b[0;32m     13\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m num_objects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Adjust this value based on your specific use case\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Create KCF trackers\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m trackers \u001b[38;5;241m=\u001b[39m [\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrackerKCF_create\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_objects)]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Read the first frame\u001b[39;00m\n\u001b[0;32m     13\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerKCF_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Assuming you have initialized 'cap' appropriately\n",
    "cap = cv2.VideoCapture('videos/influence.mp4')  # Replace 'your_video_file.mp4' with the actual video file\n",
    "\n",
    "# Define the number of objects you want to track\n",
    "num_objects = 2  # Adjust this value based on your specific use case\n",
    "\n",
    "# Create KCF trackers\n",
    "trackers = [cv2.TrackerKCF_create() for _ in range(num_objects)]\n",
    "\n",
    "# Read the first frame\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if not ret:\n",
    "    print(\"Error reading the video feed\")\n",
    "    exit()\n",
    "\n",
    "# Select the regions of interest (ROIs) for each object to track\n",
    "# You should set these coordinates based on your specific use case\n",
    "# For example, manually draw bounding boxes around objects or use an object detection model\n",
    "\n",
    "# Example ROIs\n",
    "rois = [(x1, y1, w1, h1), (x2, y2, w2, h2)]  # Adjust these values based on your specific use case\n",
    "\n",
    "# Initialize trackers with the selected ROIs\n",
    "for i, tracker in enumerate(trackers):\n",
    "    tracker.init(frame, rois[i])\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        ret, bbox = tracker.update(frame)\n",
    "\n",
    "        if ret:\n",
    "            p1 = (int(bbox[0]), int(bbox[1]))\n",
    "            p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "            cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow('Multi-object Tracking', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processing with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load an image from file\n",
    "image_path = 'images/image.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply a Gaussian blur\n",
    "blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "# Perform edge detection using Canny\n",
    "edges = cv2.Canny(blurred_image, 50, 150)\n",
    "\n",
    "# Display the original and processed images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Edges', edges)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Image Processing with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load an image from file\n",
    "image_path = 'images/image.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Apply a Gaussian blur\n",
    "blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "\n",
    "# Perform edge detection using Canny\n",
    "edges = cv2.Canny(blurred_image, 50, 150)\n",
    "\n",
    "# Find contours in the edged image\n",
    "contours, _ = cv2.findContours(edges.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# Draw contours on the original image\n",
    "cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
    "\n",
    "# Display the original, processed, and contoured images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Edges', edges)\n",
    "cv2.imshow('Contours', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Update the tracker with the new frame\n",
    "    ret, bbox = tracker.update(frame)\n",
    "\n",
    "    if ret:\n",
    "        # Draw a rectangle around the tracked object\n",
    "        p1 = (int(bbox[0]), int(bbox[1]))\n",
    "        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Tracking', frame)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'TrackerKCF_create'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize the tracker (using KCF tracker in this example)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tracker \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTrackerKCF_create\u001b[49m()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Read the first frame from the video stream\u001b[39;00m\n\u001b[0;32m     10\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'TrackerKCF_create'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Create a video capture object (0 corresponds to the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize the tracker (using KCF tracker in this example)\n",
    "tracker = cv2.TrackerKCF_create()\n",
    "\n",
    "# Read the first frame from the video stream\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Prompt the user to select a bounding box around the object to be tracked\n",
    "bbox = cv2.selectROI(\"Tracking\", frame, fromCenter=False, showCrosshair=True)\n",
    "\n",
    "# Initialize the tracker with the selected bounding box\n",
    "tracker.init(frame, bbox)\n",
    "\n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Update the tracker with the new frame\n",
    "    ret, bbox = tracker.update(frame)\n",
    "\n",
    "    if ret:\n",
    "        # Draw a rectangle around the tracked object\n",
    "        p1 = (int(bbox[0]), int(bbox[1]))\n",
    "        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Tracking', frame)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\video\\src\\tracking\\detail\\tracking_feature.cpp:128: error: (-215:Assertion failed) !patchSize.empty() in function 'cv::detail::tracking::feature::CvHaarEvaluator::FeatureHaar::generateRandomFeature'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m bbox \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mselectROI(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTracking\u001b[39m\u001b[38;5;124m\"\u001b[39m, frame, fromCenter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, showCrosshair\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize the tracker with the selected bounding box\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Read a new frame\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\video\\src\\tracking\\detail\\tracking_feature.cpp:128: error: (-215:Assertion failed) !patchSize.empty() in function 'cv::detail::tracking::feature::CvHaarEvaluator::FeatureHaar::generateRandomFeature'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Create a video capture object (0 corresponds to the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize the tracker (using MIL tracker in this example)\n",
    "tracker = cv2.TrackerMIL_create()\n",
    "\n",
    "# Read the first frame from the video stream\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Prompt the user to select a bounding box around the object to be tracked\n",
    "bbox = cv2.selectROI(\"Tracking\", frame, fromCenter=False, showCrosshair=True)\n",
    "\n",
    "# Initialize the tracker with the selected bounding box\n",
    "tracker.init(frame, bbox)\n",
    "\n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Update the tracker with the new frame\n",
    "    ret, bbox = tracker.update(frame)\n",
    "\n",
    "    if ret:\n",
    "        # Draw a rectangle around the tracked object\n",
    "        p1 = (int(bbox[0]), int(bbox[1]))\n",
    "        p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "        cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Tracking', frame)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\video\\src\\tracking\\detail\\tracking_feature.cpp:128: error: (-215:Assertion failed) !patchSize.empty() in function 'cv::detail::tracking::feature::CvHaarEvaluator::FeatureHaar::generateRandomFeature'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Initialize a new tracker with the selected bounding box\u001b[39;00m\n\u001b[0;32m     22\u001b[0m tracker \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mTrackerMIL_create()\n\u001b[1;32m---> 23\u001b[0m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Add the new tracker to the list\u001b[39;00m\n\u001b[0;32m     26\u001b[0m trackers\u001b[38;5;241m.\u001b[39mappend(tracker)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\video\\src\\tracking\\detail\\tracking_feature.cpp:128: error: (-215:Assertion failed) !patchSize.empty() in function 'cv::detail::tracking::feature::CvHaarEvaluator::FeatureHaar::generateRandomFeature'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Create a video capture object (0 corresponds to the default camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize a list to store multiple trackers\n",
    "trackers = []\n",
    "\n",
    "while True:\n",
    "    # Read a new frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # If there are no trackers or user wants to add more\n",
    "    if not trackers or cv2.waitKey(1) & 0xFF == ord('a'):\n",
    "        # Prompt the user to select a bounding box around the object to be tracked\n",
    "        bbox = cv2.selectROI(\"Tracking\", frame, fromCenter=False, showCrosshair=True)\n",
    "\n",
    "        # Initialize a new tracker with the selected bounding box\n",
    "        tracker = cv2.TrackerMIL_create()\n",
    "        tracker.init(frame, bbox)\n",
    "\n",
    "        # Add the new tracker to the list\n",
    "        trackers.append(tracker)\n",
    "\n",
    "    # Update all trackers with the new frame\n",
    "    for tracker in trackers:\n",
    "        ret, bbox = tracker.update(frame)\n",
    "\n",
    "        if ret:\n",
    "            # Draw a rectangle around the tracked object\n",
    "            p1 = (int(bbox[0]), int(bbox[1]))\n",
    "            p2 = (int(bbox[0] + bbox[2]), int(bbox[1] + bbox[3]))\n",
    "            cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Tracking', frame)\n",
    "\n",
    "    # Exit if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Processing with OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Open a video capture object\n",
    "video_path = 'videos/influence.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Read and process frames from the video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Perform some processing on each frame (e.g., convert to grayscale)\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the original and processed frames\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Grayscale Frame', gray_frame)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection with OpenCV and a Pre-trained Model (Using MobileNet SSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time Face Detection with OpenCV and Haarcascades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmentation with OpenCV (Using GrabCut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load an image from file\n",
    "image_path = 'images/image.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Create a mask (1 for foreground, 2 for probable foreground, 0 for background, 3 for probable background)\n",
    "mask = np.zeros(image.shape[:2], np.uint8)\n",
    "\n",
    "# Define the region of interest (ROI) for segmentation\n",
    "rect = (50, 50, image.shape[1] - 50, image.shape[0] - 50)\n",
    "\n",
    "# Apply GrabCut algorithm\n",
    "cv2.grabCut(image, mask, rect, None, None, 5, cv2.GC_INIT_WITH_RECT)\n",
    "\n",
    "# Modify the mask to create a binary mask (0 and 2 for background, 1 and 3 for foreground)\n",
    "mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "\n",
    "# Multiply the image with the binary mask to get the segmented image\n",
    "result = image * mask2[:, :, np.newaxis]\n",
    "\n",
    "# Display the original and segmented images\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Segmented Image', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background Subtraction for Moving Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open a video capture object\n",
    "video_path = 'videos/influence.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Create a background subtractor\n",
    "fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "# Read and process frames from the video\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply background subtraction\n",
    "    fgmask = fgbg.apply(frame)\n",
    "\n",
    "    # Display the original frame and the foreground mask\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Foreground Mask', fgmask)\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optical Flow for Motion Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open a video capture object\n",
    "video_path = 'videos/influence.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if the video capture object is successfully opened\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "\n",
    "# Convert the first frame to grayscale\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "hsv = np.zeros_like(frame1)\n",
    "hsv[..., 1] = 255\n",
    "\n",
    "while True:\n",
    "    ret, frame2 = cap.read()\n",
    "\n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the current frame to grayscale\n",
    "    next_frame = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Convert polar coordinates to Cartesian\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    hsv[..., 0] = ang * 180 / np.pi / 2\n",
    "    hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Convert HSV to BGR for visualization\n",
    "    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # Display the optical flow\n",
    "    cv2.imshow('Optical Flow', rgb)\n",
    "\n",
    "    # Update the previous frame\n",
    "    prvs = next_frame\n",
    "\n",
    "    # Break the loop if 'q' key is pressed\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Segmentation with a Pre-trained Deep Learning Model (Using DeepLabv3+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facial Recognition with OpenCV and dlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pose Estimation with OpenCV and PoseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Stitching with OpenCV (Panorama Creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read a series of images for stitching\n",
    "image1 = cv2.imread('images/image.jpg')\n",
    "image2 = cv2.imread('images/image.jpeg')\n",
    "\n",
    "# Convert images to grayscale\n",
    "gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect key points and descriptors using ORB\n",
    "orb = cv2.ORB_create()\n",
    "keypoints1, descriptors1 = orb.detectAndCompute(gray1, None)\n",
    "keypoints2, descriptors2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "# Use a brute-force matcher to find the best matches\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "# Sort the matches based on their distances\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "# Extract the matched key points\n",
    "points1 = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "points2 = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "# Find the homography matrix\n",
    "H, _ = cv2.findHomography(points1, points2, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Warp the first image to align with the second image\n",
    "result = cv2.warpPerspective(image1, H, (image1.shape[1] + image2.shape[1], image1.shape[0]))\n",
    "\n",
    "# Overlay the second image onto the result\n",
    "result[0:image2.shape[0], 0:image2.shape[1]] = image2\n",
    "\n",
    "# Display the original images and the stitched panorama\n",
    "cv2.imshow('Image 1', image1)\n",
    "cv2.imshow('Image 2', image2)\n",
    "cv2.imshow('Stitched Panorama', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Tracking with OpenCV (Using Mean Shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Inpainting with OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time Emotion Detection with OpenCV and a Pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Recognition with Tesseract OCR and OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style Transfer with Neural Style Transfer Algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
